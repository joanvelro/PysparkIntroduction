{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_hgOicJaiqsX"
   },
   "source": [
    "# Data Science with PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i6g4vOz-iqsv"
   },
   "source": [
    "Instrucciones: Lea cuidadosamente las preguntas, escriba el código correspondiente y ejecútelo para mostrar sus resultados.\n",
    "\n",
    "Antes de continuar:\n",
    "\n",
    "Asegúrese de contar con los siguientes componentes instalados en la computadora que realizará el examen.\n",
    "\n",
    "1. Python - Se recomienda Python 3.7 pues en algunos casos Python 3.8 causa algunos problemas de compatibilidad con PySpark\n",
    "2. PySpark - Se recomienda PySpark 2.x\n",
    "3. Jupyter Notebook - Se recomienda instalar la distribución Anaconda3 - 2020.02 que contiene Python 3.7. Esto puede descargarse de https://repo.anaconda.com/archive/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kdYFRiPiqsy"
   },
   "source": [
    "Importante: Todos los ejercicios deberán realizarse con funciones de NumPy, Pandas o PySpark (no podrán crearse vistas temporales para realizarse en SQL, salvo que se indique lo contrario)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:26:16.749190Z",
     "start_time": "2021-02-03T10:26:16.739426Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as sf\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0kY5gzb9iqsz"
   },
   "source": [
    "# Spark Core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YamD_3TAiqs0"
   },
   "source": [
    "1.1 Utilizando NumPy, construya un arreglo con 50 elementos aleatorios distribuidos de forma normal con media 50 y desviación estándar 10. Imprima el arreglo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:26:18.333737Z",
     "start_time": "2021-02-03T10:26:18.327668Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hYJeiGmyiyx7",
    "outputId": "5b181ef9-9a37-465d-a57f-d5113ffe1bc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[67.64052346 54.00157208 59.78737984 72.40893199 68.6755799  40.2272212\n",
      " 59.50088418 48.48642792 48.96781148 54.10598502 51.44043571 64.54273507\n",
      " 57.61037725 51.21675016 54.43863233 53.33674327 64.94079073 47.94841736\n",
      " 53.13067702 41.45904261 24.47010184 56.53618595 58.64436199 42.5783498\n",
      " 72.69754624 35.45634325 50.45758517 48.1281615  65.32779214 64.6935877\n",
      " 51.54947426 53.7816252  41.12214252 30.19203532 46.52087851 51.56348969\n",
      " 62.30290681 62.02379849 46.12673183 46.97697249 39.51447035 35.79982063\n",
      " 32.93729809 69.50775395 44.90347818 45.61925698 37.4720464  57.77490356\n",
      " 33.86102152 47.8725972 ]\n"
     ]
    }
   ],
   "source": [
    "a = np.random.normal(50,10,50)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nkaexEXziqs3"
   },
   "source": [
    "## Build spark core object\n",
    "Construya el objeto de Spark (Core) que le permita trabajar con objetos RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T11:04:06.395782Z",
     "start_time": "2021-02-03T11:04:06.383297Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SqasFiwij-yr",
    "outputId": "03f15089-353f-465e-da5c-76d41c7ccb67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.1\n",
      "<SparkContext master=local[*] appName=Python Spark SQL>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark SQL\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "print(spark.version)\n",
    "\n",
    "sc = spark.sparkContext\n",
    "print(sc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GjM8cq7riqs3"
   },
   "source": [
    "## Exercise 1\n",
    "Convierta el arreglo de NumPy a un RDD con 2 particiones. Muestre los primeros 5 elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:26:20.532772Z",
     "start_time": "2021-02-03T10:26:20.442913Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VjwFWNiWkyau",
    "outputId": "eebf04b0-356a-4f63-90fc-24451fe4d95d"
   },
   "outputs": [],
   "source": [
    "# Convert list to RDD\n",
    "rdd = sc.parallelize(a)\n",
    "#rdd.top(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8J_D7GPLiqs4"
   },
   "source": [
    "## Exercise 2\n",
    "1.4. Suponiendo que los datos de la lista miden grados Fahrenheit, aplique una función lambda al RDD que convierta las mediciones a grados Centígrados. Muestre los primeros 5 elementos. \n",
    "$$\n",
    "C = (F - 32) * 5 / 9\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:26:21.424268Z",
     "start_time": "2021-02-03T10:26:21.366662Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-4abvRYgmYKB",
    "outputId": "8882296b-c4f7-43ca-8152-4ba8caf09a83"
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 1 times, most recent failure: Lost task 7.0 in stage 0.0 (TID 7) (28APO7665.altran.es executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:188)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:108)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:121)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:162)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:175)\r\n\t... 14 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:188)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:108)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:121)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:162)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:175)\r\n\t... 14 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\JOSE~1.VEL\\AppData\\Local\\Temp/ipykernel_21624/791073049.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdegree\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mtop\u001b[1;34m(self, num, key)\u001b[0m\n\u001b[0;32m   1483\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mheapq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlargest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1485\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopIterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1487\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtakeOrdered\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mreduce\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    997\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 999\u001b[1;33m         \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1000\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    948\u001b[0m         \"\"\"\n\u001b[0;32m    949\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 1 times, most recent failure: Lost task 7.0 in stage 0.0 (TID 7) (28APO7665.altran.es executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:188)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:108)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:121)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:162)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:175)\r\n\t... 14 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:188)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:108)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:121)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:162)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:175)\r\n\t... 14 more\r\n"
     ]
    }
   ],
   "source": [
    "rdd = rdd.map(lambda degree: (degree-32)*5/9)\n",
    "rdd.top(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPifycZ6iqs9"
   },
   "source": [
    "## Exercise 3\n",
    "1.5. Utilice una función Lambda para mostrar únicamente las temperaturas mayores a 15 grados Centigrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:26:22.317757Z",
     "start_time": "2021-02-03T10:26:22.247779Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1hqy9timnJ75",
    "outputId": "f08032d6-c676-45b2-d03c-96674afa63f9"
   },
   "outputs": [],
   "source": [
    "rdd.filter(lambda degree: degree>15).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_CLygYJiqs9"
   },
   "source": [
    "## Exercise 4\n",
    "1.6. Calcule la temperatura media en grados Centígrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:26:23.131565Z",
     "start_time": "2021-02-03T10:26:23.042766Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YtS-jrBSpIVa",
    "outputId": "95c7e7e0-7c0c-431b-a934-f9cecf22c452"
   },
   "outputs": [],
   "source": [
    "rdd.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3luU86Ziqs-"
   },
   "source": [
    "## Exercise 5\n",
    "1.7. Obtenga las 3 temperaturas más altas en grados Centígrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:26:24.343455Z",
     "start_time": "2021-02-03T10:26:24.063900Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0f7E1pmbpQuz",
    "outputId": "23a7320f-3390-4a7d-8791-18c28d4fb3f4"
   },
   "outputs": [],
   "source": [
    "rdd.sortBy(lambda x: -x).top(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSsQdUcIiqs-"
   },
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hg2QoVWJiqs_"
   },
   "source": [
    "## Exercise 1 \n",
    "2.1. Utilizando Numpy, construya un arreglo con 50 números enteros entre 1 y 3 (1 y 3 incluidos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:26:24.958134Z",
     "start_time": "2021-02-03T10:26:24.950724Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rFJHY3bepu3c",
    "outputId": "55def6bb-8d29-424c-95d9-94faaa67e4c8"
   },
   "outputs": [],
   "source": [
    "b = np.random.uniform(1,3+1,50).astype(int)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmr_wrgfiqs_"
   },
   "source": [
    "## Exercise 2\n",
    "2.2. Construya un dataframe en Pandas utilizando los arreglos de 2.1 y 1.1. Asigne los nombres \"dia\" y \"temp\". Muestre los primeros 5 elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:26:25.731780Z",
     "start_time": "2021-02-03T10:26:25.720261Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "ja9Aa4mNp4Wk",
    "outputId": "b6ec3a1f-479d-41bf-a097-cfb8e58ce706"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'dia':b, 'temp':a})\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZ3xoT32iqtA"
   },
   "source": [
    "## Exercise 3\n",
    "2.3. Construya el objeto de Spark (SQL) que le permita trabajar con los dataframes de Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T11:04:58.968083Z",
     "start_time": "2021-02-03T11:04:58.959044Z"
    },
    "id": "SC_iZyuRqcvx"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "print(sqlContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9_VGuyyiqtA"
   },
   "source": [
    "## Exercise 4\n",
    "2.4. Convierta el dataframe de Pandas a un dataframe de Spark, definiendo explícitamente el esquema/estructura (utilice el tipo entero para el día y el tipo doble para la temperatura). Muestre los primeros 5 registros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:26:28.897562Z",
     "start_time": "2021-02-03T10:26:28.799841Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cPROOkijqkyU",
    "outputId": "a1bc91ee-7481-407d-9c89-09842545c2d8"
   },
   "outputs": [],
   "source": [
    "schemaString = \"dia temp\"\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "schema = StructType(fields)\n",
    "sqlContext.createDataFrame(df, schema).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oc_ICQnuiqtD"
   },
   "source": [
    "## Exercise 5 \n",
    "2.5. Partiendo del dataframe en Spark, construya un dataframe con el promedio de temperatura agrupado por día. El dataframe deberá contener únicamente las columnas \"dia\" y \"temp_prom\" (con esos nombres). Muestre la tabla resultante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:26:29.911108Z",
     "start_time": "2021-02-03T10:26:29.879642Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf = sqlContext.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:26:30.307430Z",
     "start_time": "2021-02-03T10:26:30.301499Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:26:30.868996Z",
     "start_time": "2021-02-03T10:26:30.704424Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:26:31.926308Z",
     "start_time": "2021-02-03T10:26:31.222300Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rKaV0yhHrHKM",
    "outputId": "651c539f-3a40-4838-acc1-0e2704f76ff1"
   },
   "outputs": [],
   "source": [
    "sdf.groupBy('dia').mean().select('dia','avg(temp)').withColumnRenamed('avg(temp)', 'temp_prom').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjMDMUx5iqtE"
   },
   "source": [
    "## Exercise 6\n",
    "2.6. Repita el ejercicio anterior registrando una vista temporal y ejecutando el código SQL correspondiente. Muestre la tabla resultante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:26:40.814052Z",
     "start_time": "2021-02-03T10:26:40.810753Z"
    },
    "id": "zPq5anWessg2"
   },
   "outputs": [],
   "source": [
    "sdf.createGlobalTempView(\"temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:26:32.851450Z",
     "start_time": "2021-02-03T10:26:32.848249Z"
    }
   },
   "outputs": [],
   "source": [
    "query=\"\"\"\n",
    "SELECT\n",
    "    dia,\n",
    "    AVG(temp) as temp_prom\n",
    "FROM\n",
    "    global_temp.temp\n",
    "GROUP BY\n",
    "    dia\n",
    "ORDER BY\n",
    "    dia\n",
    "ASC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:26:33.320383Z",
     "start_time": "2021-02-03T10:26:33.316722Z"
    }
   },
   "outputs": [],
   "source": [
    "#spark.sql(query).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:26:45.087216Z",
     "start_time": "2021-02-03T10:26:44.360490Z"
    }
   },
   "outputs": [],
   "source": [
    "sqlContext.sql(query).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qy24N7zjiqtF"
   },
   "source": [
    "## Exercise 7\n",
    "2.7. Combine los valores del dataframe anterior con el original. El dataframe resultante no deberá contener columnas repetidas y tendrá que estar ordenado de forma ascendente por día y temperatura. Muestre los primeros 5 elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:26:46.522522Z",
     "start_time": "2021-02-03T10:26:46.518721Z"
    }
   },
   "outputs": [],
   "source": [
    "query2 = \"\"\"\n",
    "SELECT\n",
    "    T.dia,\n",
    "    T.temp\n",
    "FROM global_temp.temp as T\n",
    "LEFT JOIN\n",
    "(SELECT dia, AVG(temp) as temp FROM global_temp.temp GROUP BY dia) as A \n",
    "ON\n",
    "    T.dia=A.dia\n",
    "ORDER BY\n",
    "    dia,\n",
    "    temp\n",
    "DESC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:26:48.481589Z",
     "start_time": "2021-02-03T10:26:47.109822Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "guV9wWJNvYnX",
    "outputId": "8b52b664-ceb9-4946-a6ae-f1aa2376ee78"
   },
   "outputs": [],
   "source": [
    "sqlContext.sql(query2).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fcub0VxniqtH"
   },
   "source": [
    "## Exercise 8\n",
    "2.8. Añada una columna adicicional con la diferencia entre la temperatura y su media. Asigne el nombre \"resid\". Muestre los primeros 5 elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:26:48.585744Z",
     "start_time": "2021-02-03T10:26:48.075Z"
    }
   },
   "outputs": [],
   "source": [
    "query3 = \"\"\"\n",
    "SELECT\n",
    "    T.dia as dia,\n",
    "    T.temp as t_obs,\n",
    "    A.temp as t_avg,\n",
    "    T.temp - A.temp as t_resid\n",
    "FROM global_temp.temp as T\n",
    "LEFT JOIN\n",
    "(SELECT dia, AVG(temp) as temp FROM global_temp.temp GROUP BY dia) as A \n",
    "ON\n",
    "    T.dia=A.dia\n",
    "ORDER BY\n",
    "    dia,\n",
    "    t_obs,\n",
    "    t_avg,\n",
    "    t_resid\n",
    "DESC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:26:50.230292Z",
     "start_time": "2021-02-03T10:26:48.711318Z"
    }
   },
   "outputs": [],
   "source": [
    "sqlContext.sql(query3).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-jytBAgiqtI"
   },
   "source": [
    "## Exercise 9\n",
    "2.9. Construya un dataframe con todos los registros que posean residuales negativos. Muestre los primeros 5 elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:26:50.338378Z",
     "start_time": "2021-02-03T10:26:49.560Z"
    }
   },
   "outputs": [],
   "source": [
    "query4 = \"\"\"\n",
    "SELECT\n",
    "    *\n",
    "FROM (\n",
    "SELECT\n",
    "    T.dia as dia,\n",
    "    T.temp as t_obs,\n",
    "    A.temp as t_avg,\n",
    "    T.temp - A.temp as t_resid\n",
    "FROM global_temp.temp as T\n",
    "LEFT JOIN\n",
    "(SELECT dia, AVG(temp) as temp FROM global_temp.temp GROUP BY dia) as A \n",
    "ON\n",
    "    T.dia=A.dia\n",
    "ORDER BY\n",
    "    t_resid\n",
    "ASC\n",
    ") as P\n",
    "WHERE \n",
    "    t_resid < 0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:26:50.340643Z",
     "start_time": "2021-02-03T10:26:50.129Z"
    }
   },
   "outputs": [],
   "source": [
    "sqlContext.sql(query4).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:26:52.045069Z",
     "start_time": "2021-02-03T10:26:50.953730Z"
    },
    "id": "XWWNuJp_yNkz"
   },
   "outputs": [],
   "source": [
    "df_temp = sqlContext.sql(query4)\n",
    "df_temp.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fy9gOEUiqtI"
   },
   "source": [
    "## Exercise 10\n",
    "2.10. Guarde el dataframe resultante en formato JSON. En caso de que el archivo ya exista, deberá sobreescribirse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:27:00.590021Z",
     "start_time": "2021-02-03T10:26:54.866595Z"
    }
   },
   "outputs": [],
   "source": [
    "df_temp.select(\"t_resid\", \"dia\").write.format('com.databricks.spark.csv') \\\n",
    "  .mode('overwrite').option(\"header\", \"true\").save(\"temp.json\",format=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:27:00.890281Z",
     "start_time": "2021-02-03T10:26:56.393Z"
    }
   },
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-OLudzciqtL"
   },
   "source": [
    "# Spark MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBG1JNy0iqtM"
   },
   "source": [
    "En esta sección se evalúan los conocimientos de Spark MLlib. Si bien son necesarios los conociemientos en Machine Learning, el candidato no será evaluado por la calidad del modelo producido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QC_YTZJEiqtN"
   },
   "source": [
    "## Load data\n",
    "load data from _data.csv_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:27:04.706836Z",
     "start_time": "2021-02-03T10:27:04.700646Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fpEr7T9dzO85",
    "outputId": "8ccbea9e-41ff-4193-d3bf-12e3f76970ca"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T11:05:24.948406Z",
     "start_time": "2021-02-03T11:05:24.740128Z"
    }
   },
   "outputs": [],
   "source": [
    "df_data = spark.read.csv(path='data.csv',\n",
    "                         sep=',',\n",
    "                         encoding='UTF-8',\n",
    "                         comment=None, \n",
    "                         header=True, \n",
    "                         inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T11:05:25.985702Z",
     "start_time": "2021-02-03T11:05:25.621519Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Memory usage: {} MB'.format(df_data.toPandas().memory_usage().sum()/1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T11:05:26.801976Z",
     "start_time": "2021-02-03T11:05:26.499427Z"
    }
   },
   "outputs": [],
   "source": [
    "df_data.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:27:11.472911Z",
     "start_time": "2021-02-03T10:27:11.312906Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df_data.limit(2).collect() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:27:12.511160Z",
     "start_time": "2021-02-03T10:27:12.359497Z"
    }
   },
   "outputs": [],
   "source": [
    "df_data.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GpNiL9viqtN"
   },
   "source": [
    "## EDA \n",
    "estadísticos básicos de las columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:27:14.935274Z",
     "start_time": "2021-02-03T10:27:14.215887Z"
    }
   },
   "outputs": [],
   "source": [
    "df_data.toPandas().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:27:19.193207Z",
     "start_time": "2021-02-03T10:27:18.662314Z"
    }
   },
   "outputs": [],
   "source": [
    "df_data.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert timestamp into variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T11:08:23.544741Z",
     "start_time": "2021-02-03T11:08:23.482691Z"
    }
   },
   "outputs": [],
   "source": [
    "df_data = df_data.withColumn(\"year\", sf.year(sf.col(\"Onboard_date\")))\n",
    "df_data = df_data.withColumn(\"month\", sf.month(sf.col(\"Onboard_date\")))\n",
    "df_data = df_data.withColumn(\"day\", sf.dayofmonth(sf.col(\"Onboard_date\")))\n",
    "df_data = df_data.withColumn(\"weekday\", sf.dayofweek(sf.col(\"Onboard_date\")))\n",
    "df_data = df_data.withColumn(\"hour\", sf.hour(sf.col(\"Onboard_date\")))\n",
    "df_data = df_data.withColumn(\"minute\", sf.minute(sf.col(\"Onboard_date\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:28:02.649539Z",
     "start_time": "2021-02-03T10:28:01.755727Z"
    }
   },
   "outputs": [],
   "source": [
    "df_data.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T11:08:25.609080Z",
     "start_time": "2021-02-03T11:08:25.601034Z"
    }
   },
   "outputs": [],
   "source": [
    "df_data = df_data.drop('Onboard_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:28:08.079310Z",
     "start_time": "2021-02-03T10:28:08.058775Z"
    }
   },
   "outputs": [],
   "source": [
    "df_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot distributions of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:28:18.397183Z",
     "start_time": "2021-02-03T10:28:09.047283Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,15)) ## Plot Size \n",
    "st = fig.suptitle(\"Distribution of Features\", fontsize=50,\n",
    "                  verticalalignment='center') # Plot Main Title \n",
    "\n",
    "for col,num in zip(df_data.toPandas().describe().columns, range(1,11)):\n",
    "    ax = fig.add_subplot(3,4,num)\n",
    "    ax.hist(df_data.toPandas()[col])\n",
    "    #plt.style.use('dark_background') \n",
    "    plt.grid(False)\n",
    "    plt.xticks(rotation=45,fontsize=20)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.title(col.upper(),fontsize=20)\n",
    "plt.tight_layout()\n",
    "st.set_y(0.95)\n",
    "fig.subplots_adjust(top=0.85,hspace = 0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:28:22.929911Z",
     "start_time": "2021-02-03T10:28:22.362346Z"
    }
   },
   "outputs": [],
   "source": [
    "numeric_features = [t[0] for t in df_data.dtypes if t[1] != 'string']\n",
    "numeric_features_df = df_data.select(numeric_features)\n",
    "numeric_features_df.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:28:28.628552Z",
     "start_time": "2021-02-03T10:28:28.273184Z"
    }
   },
   "outputs": [],
   "source": [
    "categorical_features = [t[0] for t in df_data.dtypes if t[1] == 'string']\n",
    "categorical_features_df = df_data.select(categorical_features)\n",
    "categorical_features_df.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:28:36.882013Z",
     "start_time": "2021-02-03T10:28:34.908072Z"
    }
   },
   "outputs": [],
   "source": [
    "col_names = numeric_features_df.columns\n",
    "features = numeric_features_df.rdd.map(lambda row: row[0:])\n",
    "corr_mat = Statistics.corr(features, method=\"pearson\")\n",
    "corr_df = pd.DataFrame(corr_mat)\n",
    "corr_df.index, corr_df.columns = col_names, col_names\n",
    "\n",
    "corr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:28:41.203293Z",
     "start_time": "2021-02-03T10:28:40.753330Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = go.Figure(data=go.Heatmap(\n",
    "                   z=corr_df.values,\n",
    "                   x=corr_df.columns,\n",
    "                   y=corr_df.columns,\n",
    "                   hoverongaps = False))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:28:51.897855Z",
     "start_time": "2021-02-03T10:28:50.498636Z"
    }
   },
   "outputs": [],
   "source": [
    "df_data.select([sf.count(sf.when(sf.isnan(c), c)).alias(c) for c in df_data.columns]).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T13:54:57.951716Z",
     "start_time": "2021-02-01T13:54:57.947590Z"
    }
   },
   "source": [
    "## method 1: Transformations step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index Categorical features\n",
    "Convert categorical variables into indexed ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:28:58.118500Z",
     "start_time": "2021-02-03T10:28:58.110362Z"
    }
   },
   "outputs": [],
   "source": [
    "df_aux0 = df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:28:59.624383Z",
     "start_time": "2021-02-03T10:28:59.614560Z"
    }
   },
   "outputs": [],
   "source": [
    "print(categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:29:02.891264Z",
     "start_time": "2021-02-03T10:29:00.356947Z"
    }
   },
   "outputs": [],
   "source": [
    "for categoricalCol in categorical_features:\n",
    "    stringIndexer = StringIndexer(inputCol= categoricalCol,\n",
    "                                  outputCol= categoricalCol + \"_index\")\n",
    "\n",
    "    Index_model = stringIndexer.fit(df_aux0)\n",
    "    df_aux0 = Index_model.transform(df_aux0)\n",
    "\n",
    "df_aux0.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode categorical indexed variables\n",
    "The categorical variables indexed are now encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:29:09.035082Z",
     "start_time": "2021-02-03T10:29:07.266827Z"
    }
   },
   "outputs": [],
   "source": [
    "df_aux1 = df_aux0\n",
    "for feature in categorical_features:\n",
    "    encoder = OneHotEncoderEstimator()\\\n",
    "                .setInputCols([feature + '_index'])\\\n",
    "                .setOutputCols([feature +  \"_coded\"])\n",
    "\n",
    "    encoder_model = encoder.fit(df_aux1)\n",
    "    df_aux1 = encoder_model.transform(df_aux1)\n",
    "\n",
    "df_aux1.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector assembler\n",
    "Vectorize both numerical and encoded categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:29:11.912445Z",
     "start_time": "2021-02-03T10:29:11.898446Z"
    }
   },
   "outputs": [],
   "source": [
    "input_features = [c + '_coded' for c in categorical_features]\n",
    "input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:29:14.510692Z",
     "start_time": "2021-02-03T10:29:14.502923Z"
    }
   },
   "outputs": [],
   "source": [
    "numeric_features.remove('Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:29:15.644392Z",
     "start_time": "2021-02-03T10:29:15.627750Z"
    }
   },
   "outputs": [],
   "source": [
    "assemblerInputs = [c for c in input_features] + numeric_features\n",
    "assemblerInputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:29:19.569284Z",
     "start_time": "2021-02-03T10:29:17.447253Z"
    }
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler()\\\n",
    "         .setInputCols (assemblerInputs)\\\n",
    "         .setOutputCol (\"vectorized_features\")\n",
    "        \n",
    "\n",
    "df_aux2 = assembler.transform(df_aux1)\n",
    "df_aux2.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:29:24.517929Z",
     "start_time": "2021-02-03T10:29:23.636241Z"
    }
   },
   "outputs": [],
   "source": [
    "label_indexer = StringIndexer()\\\n",
    "         .setInputCol (\"Churn\")\\\n",
    "         .setOutputCol (\"label\")\n",
    "\n",
    "label_indexer_model = label_indexer.fit(df_aux2)\n",
    "df_aux3 = label_indexer_model.transform(df_aux2)\n",
    "\n",
    "df_aux3.select(\"Churn\",\"label\").toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale features \n",
    "To make them vary in the same range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:54:40.467024Z",
     "start_time": "2021-02-03T10:54:39.522682Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\\\n",
    "         .setInputCol (\"vectorized_features\")\\\n",
    "         .setOutputCol (\"features\")\n",
    "        \n",
    "scaler_model = scaler.fit(df_aux3)\n",
    "df_aux4 = scaler_model.transform(df_aux3)\n",
    "\n",
    "df_aux4.select(\"vectorized_features\",\"features\").toPandas().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:54:44.099586Z",
     "start_time": "2021-02-03T10:54:43.216167Z"
    }
   },
   "outputs": [],
   "source": [
    "df_aux4.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:30:36.819117Z",
     "start_time": "2021-02-03T10:30:36.804410Z"
    }
   },
   "outputs": [],
   "source": [
    "df_aux4.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Pipeline transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:30:39.234472Z",
     "start_time": "2021-02-03T10:30:39.129195Z"
    }
   },
   "outputs": [],
   "source": [
    "categorical_features = [t[0] for t in df_data.dtypes if t[1] == 'string']\n",
    "\n",
    "stringIndexer1 = StringIndexer()\\\n",
    "                 .setInputCol (\"Names\")\\\n",
    "                 .setOutputCol (\"Names_index\")\n",
    "stringIndexer2 = StringIndexer()\\\n",
    "                 .setInputCol (\"Location\")\\\n",
    "                 .setOutputCol (\"Location_index\")\n",
    "stringIndexer3 = StringIndexer()\\\n",
    "                 .setInputCol (\"Company\")\\\n",
    "                 .setOutputCol (\"Company_index\")\n",
    "\n",
    "input_index_features = [c + '_index' for c in categorical_features]\n",
    "output_coded_features = [c + '_coded' for c in categorical_features]\n",
    "\n",
    "encoder = OneHotEncoderEstimator()\\\n",
    "         .setInputCols (input_index_features)\\\n",
    "         .setOutputCols (output_coded_features)\n",
    "\n",
    "numerical_features = [t[0] for t in df_data.dtypes if t[1] != 'string']\n",
    "numerical_features.remove('Churn')\n",
    "assemblerInputs = [c for c in output_coded_features] + numerical_features\n",
    "\n",
    "assembler = VectorAssembler()\\\n",
    "         .setInputCols (assemblerInputs)\\\n",
    "         .setOutputCol (\"vectorized_features\")\n",
    "\n",
    "label_indexer = StringIndexer()\\\n",
    "         .setInputCol (\"Churn\")\\\n",
    "         .setOutputCol (\"label\")\n",
    "\n",
    "scaler = StandardScaler()\\\n",
    "         .setInputCol (\"vectorized_features\")\\\n",
    "         .setOutputCol (\"features\")\n",
    "\n",
    "pipeline_stages = Pipeline().setStages([stringIndexer1,\n",
    "                                        stringIndexer2,\n",
    "                                        stringIndexer3,\n",
    "                                        encoder,\n",
    "                                        assembler,\n",
    "                                        label_indexer,\n",
    "                                        scaler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:30:43.981158Z",
     "start_time": "2021-02-03T10:30:40.944681Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline_model = pipeline_stages.fit(df_data)\n",
    "df_aux5 = pipeline_model.transform(df_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare method 1 with method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:55:52.714028Z",
     "start_time": "2021-02-03T10:55:51.854740Z"
    }
   },
   "outputs": [],
   "source": [
    "df_aux4.select(output_coded_features + input_index_features + ['features','label']).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:56:06.486185Z",
     "start_time": "2021-02-03T10:56:05.622715Z"
    }
   },
   "outputs": [],
   "source": [
    "df_aux5.select(output_coded_features + input_index_features + ['features','label']).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-01T14:12:03.050436Z",
     "start_time": "2021-02-01T14:12:03.047049Z"
    }
   },
   "source": [
    "## features/target split\n",
    "Realice la separación en los conjuntos de entrenamiento y prueba con una proporción 70-30. Muestre los primeros 5 elementos de cada conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:30:51.867411Z",
     "start_time": "2021-02-03T10:30:51.840703Z"
    }
   },
   "outputs": [],
   "source": [
    "df_T = df_aux5.select('features', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:30:52.980955Z",
     "start_time": "2021-02-03T10:30:52.579585Z"
    }
   },
   "outputs": [],
   "source": [
    "y = df_T.select('label')\n",
    "y.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:30:54.433856Z",
     "start_time": "2021-02-03T10:30:53.481415Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df_T.select('features')\n",
    "X.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pQhPBPDiqtO"
   },
   "source": [
    "## Train-test split \n",
    "\n",
    "\n",
    "Separar features y target \n",
    "Obtenga el conjunto de datos con el vector de variables independientes y la variable dependiente (churn). Por simplicidad, es suficiente que seleccione únicamente las variables numéricas. Muestre los primeros 5 elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:56:36.410032Z",
     "start_time": "2021-02-03T10:56:35.892353Z"
    }
   },
   "outputs": [],
   "source": [
    "train, test = df_T.randomSplit([0.7, 0.3], seed = 2018)\n",
    "print(\"Training Dataset Count:{} \".format(str(train.count())))\n",
    "print(\"Test Dataset Count:{} \".format(str(test.count())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:47:42.300099Z",
     "start_time": "2021-02-03T10:47:40.237874Z"
    }
   },
   "outputs": [],
   "source": [
    "train.groupby(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:47:45.410971Z",
     "start_time": "2021-02-03T10:47:43.945801Z"
    }
   },
   "outputs": [],
   "source": [
    "test.groupby(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-HKMGZ1iqtP"
   },
   "source": [
    "## Model training \n",
    "Ajuste un modelo de regresión logística \n",
    "con los hiperparámetros por defecto. Muestre los estadísticos descriptivos de las predicciones contenidas en el resumen del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:57:35.532460Z",
     "start_time": "2021-02-03T10:57:35.521947Z"
    }
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol = 'features',\n",
    "                        labelCol = 'label',\n",
    "                        maxIter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:57:38.308879Z",
     "start_time": "2021-02-03T10:57:37.088089Z"
    }
   },
   "outputs": [],
   "source": [
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:48:15.876174Z",
     "start_time": "2021-02-03T10:48:15.780123Z"
    }
   },
   "outputs": [],
   "source": [
    "trainingSummary = lrModel.summary\n",
    "pr = trainingSummary.pr.toPandas()\n",
    "tp = trainingSummary.precisionByThreshold.toPandas()\n",
    "fm = trainingSummary.fMeasureByThreshold.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T11:10:01.150300Z",
     "start_time": "2021-02-03T11:10:01.145055Z"
    }
   },
   "source": [
    "<img src=\"images/Precisionrecall.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Precision__\n",
    "\n",
    "precision is the number of correctly identified positive results divided by the number of all positive results, including those not identified correctly, and the  Precision is also known as positive predictive value. Indicate how many items selected are relevant\n",
    "\n",
    "$$\n",
    "Precision = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "TP: True Positive\n",
    "FP: False Positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Precision.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:48:18.384847Z",
     "start_time": "2021-02-03T10:48:18.152183Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(tp['threshold'], tp['precision'])\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:48:20.167927Z",
     "start_time": "2021-02-03T10:48:19.943643Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(pr['recall'], pr['precision'])\n",
    "plt.xlabel('recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Recall__\n",
    "\n",
    "recall is the number of correctly identified positive results divided by the number of all samples that should have been identified as positive.\n",
    "recall is also known as sensitivity in diagnostic binary classification. Indicate how many relevant items selected are selected\n",
    "\n",
    "$$\n",
    "Recall = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "TP: True Positive\n",
    "FN: False Negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/recall.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:49:57.941976Z",
     "start_time": "2021-02-03T10:49:57.938702Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the threshold-recall curve\n",
    "#plt.plot(tp['threshold'], pr['recall'])\n",
    "#plt.xlabel('Threshold')\n",
    "#plt.ylabel('Recall')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:49:59.186433Z",
     "start_time": "2021-02-03T10:49:58.960167Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the recall-precision curve\n",
    "plt.plot(pr['recall'], pr['precision'])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__F-score ($F_1$)__\n",
    "\n",
    "It measure of a test's accuracy. It is calculated from the precision and recall\n",
    "\n",
    "$$\n",
    "F_1 =2 \\cdot \\frac{precision \\cdot recall}{precision + recall}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:50:01.362771Z",
     "start_time": "2021-02-03T10:50:01.133274Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the threshold-F-Measure curve \n",
    "plt.plot(fm['threshold'], fm['F-Measure'])\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('F-1 Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get predictions\n",
    "Evalúe los resultados en el conjunto de prueba. Muestre las primeras 5 predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:50:03.458312Z",
     "start_time": "2021-02-03T10:50:03.392902Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = lrModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:50:05.101098Z",
     "start_time": "2021-02-03T10:50:04.583829Z"
    }
   },
   "outputs": [],
   "source": [
    "#predictions_train = lrModel.transform(train)\n",
    "predictions.select('label', 'features',  'rawPrediction', 'prediction', 'probability').toPandas().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:50:09.499646Z",
     "start_time": "2021-02-03T10:50:09.487232Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:50:11.838105Z",
     "start_time": "2021-02-03T10:50:10.903565Z"
    }
   },
   "outputs": [],
   "source": [
    "y_true = predictions.select(\"label\")\n",
    "y_true = y_true.toPandas()\n",
    "\n",
    "y_pred = predictions.select(\"prediction\")\n",
    "y_pred = y_pred.toPandas()\n",
    "\n",
    "class_names = [1.0,0.0]\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_true, y_pred, labels=class_names)\n",
    "\n",
    "#cnf_matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix,\n",
    "                      classes=class_names,\n",
    "                      title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:50:14.573800Z",
     "start_time": "2021-02-03T10:50:13.995758Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(predictions.count())\n",
    "print(\"Accuracy : {} % \".format(round(100*accuracy,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve\n",
    "Para evaluar el desempeño del modelo, obtenga el valor del indicador auROC (área debajo de la curva ROC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T10:50:16.345675Z",
     "start_time": "2021-02-03T10:50:15.446860Z"
    }
   },
   "outputs": [],
   "source": [
    "trainingSummary = lrModel.summary\n",
    "roc = trainingSummary.roc.toPandas()\n",
    "\n",
    "print('Train Area Under ROC: ' + str(trainingSummary.areaUnderROC))\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print('Test Area Under ROC', evaluator.evaluate(predictions))\n",
    "\n",
    "plt.plot(roc['FPR'], roc['TPR'])\n",
    "plt.ylabel('False Positive Rate')\n",
    "plt.xlabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iuq2Tg6LiqtQ"
   },
   "source": [
    "## Inference\n",
    "Repeat with new data\n",
    "Cargue los datos del archivo 'data_new.csv' y obtenga las predicciones sobre ese conjunto de datos utilizando los objetos construidos previamente. Muestre los primeros 5 elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T11:18:49.832726Z",
     "start_time": "2021-02-03T11:18:49.661616Z"
    }
   },
   "outputs": [],
   "source": [
    "df_data_new = spark.read.csv(path='data_new.csv', sep=',',encoding='UTF-8',comment=None, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T11:18:50.792852Z",
     "start_time": "2021-02-03T11:18:50.715114Z"
    }
   },
   "outputs": [],
   "source": [
    "df_data_new.toPandas().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T11:18:56.298981Z",
     "start_time": "2021-02-03T11:18:56.221559Z"
    }
   },
   "outputs": [],
   "source": [
    "df_data_new.toPandas().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform timestamp in features-time based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T11:19:03.082417Z",
     "start_time": "2021-02-03T11:19:03.020325Z"
    }
   },
   "outputs": [],
   "source": [
    "df_data_new = df_data_new.withColumn(\"year\", sf.year(sf.col(\"Onboard_date\")))\n",
    "df_data_new = df_data_new.withColumn(\"month\", sf.month(sf.col(\"Onboard_date\")))\n",
    "df_data_new = df_data_new.withColumn(\"day\", sf.dayofmonth(sf.col(\"Onboard_date\")))\n",
    "df_data_new = df_data_new.withColumn(\"weekday\", sf.dayofweek(sf.col(\"Onboard_date\")))\n",
    "df_data_new = df_data_new.withColumn(\"hour\", sf.hour(sf.col(\"Onboard_date\")))\n",
    "df_data_new = df_data_new.withColumn(\"minute\", sf.minute(sf.col(\"Onboard_date\")))\n",
    "df_data_new = df_data_new.drop(\"Onboard_date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create random label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T11:19:04.409535Z",
     "start_time": "2021-02-03T11:19:04.156033Z"
    }
   },
   "outputs": [],
   "source": [
    "df_data_new = df_data_new.withColumn('Churn', sf.round(sf.rand(df_data_new.count()),1))\n",
    "y_udf = sf.udf(lambda y: 0 if y<=0.5 else 1)\n",
    "df_data_new = df_data_new.withColumn(\"Churn\", y_udf('Churn'))\n",
    "df_data_new = df_data_new.withColumn(\"Churn\", df_data_new[\"Churn\"].cast(IntegerType()))\n",
    "df_data_new.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T11:20:57.526771Z",
     "start_time": "2021-02-03T11:20:56.977080Z"
    }
   },
   "outputs": [],
   "source": [
    "df_data.groupby('Churn').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T11:20:47.845209Z",
     "start_time": "2021-02-03T11:20:47.081421Z"
    }
   },
   "outputs": [],
   "source": [
    "df_data_new.groupby('Churn').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T11:19:09.173862Z",
     "start_time": "2021-02-03T11:19:09.164211Z"
    }
   },
   "outputs": [],
   "source": [
    "df_data_new.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply again pipeline transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T11:32:41.589538Z",
     "start_time": "2021-02-03T11:32:40.692046Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline_model = pipeline_stages.fit(df_data_new)\n",
    "df_aux6 = pipeline_model.transform(df_data_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T11:33:20.871073Z",
     "start_time": "2021-02-03T11:33:20.858851Z"
    }
   },
   "outputs": [],
   "source": [
    "data_set_inference = df_aux6.select('label','features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T11:36:05.486193Z",
     "start_time": "2021-02-03T11:36:05.243548Z"
    }
   },
   "outputs": [],
   "source": [
    "inference = lrModel.transform(data_set_inference)\n",
    "\n",
    "try:\n",
    "    accuracy = inference.filter(inference.label == inference.prediction).count() / float(inference.count())\n",
    "    print(\"Accuracy : {} % \".format(round(100*accuracy,2)))\n",
    "\n",
    "    inference.select('label', 'features',  'rawPrediction', 'prediction', 'probability').toPandas().head(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### concat new data with all the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T11:26:54.163339Z",
     "start_time": "2021-02-03T11:26:54.154761Z"
    }
   },
   "outputs": [],
   "source": [
    "df_data_total = df_data.union(df_data_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T11:19:30.289340Z",
     "start_time": "2021-02-03T11:19:30.027521Z"
    }
   },
   "outputs": [],
   "source": [
    "df_data_total.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T11:19:58.136000Z",
     "start_time": "2021-02-03T11:19:57.459728Z"
    }
   },
   "outputs": [],
   "source": [
    "df_data_total.groupby(\"Churn\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T11:26:59.449856Z",
     "start_time": "2021-02-03T11:26:59.210756Z"
    }
   },
   "outputs": [],
   "source": [
    "df_data_total.filter((df_data_total.Churn!=0) & (df_data_total.Churn!=1)).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T11:27:10.942259Z",
     "start_time": "2021-02-03T11:27:10.929934Z"
    }
   },
   "outputs": [],
   "source": [
    "df_data_total = df_data_total.filter((df_data_total.Churn==0) | (df_data_total.Churn==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T11:27:20.312784Z",
     "start_time": "2021-02-03T11:27:20.017557Z"
    }
   },
   "outputs": [],
   "source": [
    "df_data_total.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T11:27:26.381718Z",
     "start_time": "2021-02-03T11:27:24.586113Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline_model = pipeline_stages.fit(df_data_total)\n",
    "df_data_total_aux = pipeline_model.transform(df_data_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T11:38:54.484212Z",
     "start_time": "2021-02-03T11:38:53.318860Z"
    }
   },
   "outputs": [],
   "source": [
    "inference = lrModel.transform(df_data_total_aux.select('features','label'))\n",
    "\n",
    "accuracy = inference.filter(inference.label == inference.prediction).count() / float(inference.count())\n",
    "print(\"Accuracy : {} % \".format(round(100*accuracy,2)))\n",
    "\n",
    "trainingSummary = lrModel.summary\n",
    "roc = trainingSummary.roc.toPandas()\n",
    "print('AUC: ' + str(round(trainingSummary.areaUnderROC,4)))\n",
    "\n",
    "inference.select('label', 'features',  'rawPrediction', 'prediction', 'probability').toPandas().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "colab": {
   "collapsed_sections": [],
   "name": "examen_pyspark.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
